{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9920864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import smtplib\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import csv\n",
    "\n",
    "import datetime\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aed64d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200g / 7.05oz 2006 Year Ripe Shu Top Aged Lucky Dragon Puerh Tea Cake - Yunnan Pu-erh Pu erh Puer Pu'er Chinese Tea\n",
      "18.98\n",
      "3.9\n"
     ]
    }
   ],
   "source": [
    "# Connect to Website\n",
    "\n",
    "URL = 'https://www.amazon.com/Yunnan-Lucky-Dragon-Pu-erh-Chinese/dp/B016DJ2S3I/ref=sr_1_9?crid=2XQILW7VQY85L&keywords=%E6%99%AE%E6%B4%B1%E8%8C%B6&qid=1705822109&refresh=1&sprefix=%E6%99%AE%E6%B4%B1%E8%8C%B6%2Caps%2C418&sr=8-9'\n",
    "\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36','Accept-Encoding': 'gzip, deflate, br','Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7','DNT':'1','Connection':'close','Upgrade-Insecure-Requests':'1'}\n",
    "\n",
    "page = requests.get(URL, headers=headers)\n",
    "\n",
    "Soup1 = BeautifulSoup(page.content,'html.parser')  #page.content 将会返回整个HTML源码。而page.text则只会返回文本内容\n",
    "\n",
    "Soup2 = BeautifulSoup(Soup1.prettify(),'html.parser') #Soup1.prettify()是格式化并美化解析后的HTML或XML内容，缩进成便于阅读的情况\n",
    "\n",
    "# print(Soup2)\n",
    "\n",
    "title = Soup2.find( id = 'productTitle' ).get_text().strip()\n",
    "\n",
    "price = Soup2.find('span', attrs = {'class':'a-offscreen'}).get_text().strip()   #.strip是为了把前后的空格删掉\n",
    "\n",
    "# rating = Soup2.find('span', attrs={'class': 'a-size-base a-color-base'}).get_text().strip()\n",
    "\n",
    "#get rating\n",
    "rating = Soup2.find('div', attrs={'id': 'averageCustomerReviews'}).find('span', attrs={'class': 'a-declarative'}).find('span', attrs={'id': 'acrPopover'}).find('span', attrs={'class': 'a-declarative'}).find('a', attrs={'class': 'a-popover-trigger a-declarative'}).find('span', attrs={'class': 'a-size-base a-color-base'}).get_text().strip()\n",
    "\n",
    "# rating = Soup2.select_one('#averageCustomerReviews > span.a-declarative > span#acrPopover > a.a-popover-trigger.a-declarative > span.a-size-base.a-color-base').get_text().strip() if Soup2 else None\n",
    "\n",
    "print(title)\n",
    "\n",
    "price = price[1:]\n",
    "print(price)\n",
    "\n",
    "# rating = rating[:3]\n",
    "print(rating)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "681325cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-22\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "today = datetime.date.today()\n",
    "\n",
    "print(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebe77e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "#新建一个csv文件,且录入第一笔数据\n",
    "\n",
    "header = ['Title','Price','Date','Rating',]\n",
    "data = [title, price, today, rating]\n",
    "\n",
    "with open('AmazonWebScrapeDataSet.csv','w',newline = '',encoding = 'UTF8') as f:\n",
    "    #'w': 这是写入模式，如果文件已存在则会覆盖原有内容，如果不存在则创建新文件。\n",
    "    # newline='' 的作用：在写入文件时，不自动添加任何特定系统的换行符，而是由编写程序的人来决定具体使用哪种换行符；在读取文件时，统一将所有类型的换行符都识别为\\n。\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    writer.writerow(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a97be457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  Price        Date  \\\n",
      "0  200g / 7.05oz 2006 Year Ripe Shu Top Aged Luck...  18.98  2024-01-22   \n",
      "1  200g / 7.05oz 2006 Year Ripe Shu Top Aged Luck...  18.98  2024-01-22   \n",
      "\n",
      "   Rating  \n",
      "0     3.9  \n",
      "1     3.9  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\47042\\AmazonWebScrapeDataSet.csv')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46d0bddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending data to the csv\n",
    "\n",
    "header = ['Title','Price','Date','Rating',]\n",
    "data = [title, price, today, rating]\n",
    "\n",
    "with open('AmazonWebScrapeDataSet.csv','a+',newline = '',encoding = 'UTF8') as f:\n",
    "    # 'a+'模式意味着你既可以读取文件现有的所有内容，也可以在文件末尾添加新的内容，但不会覆盖原有文件内容。\n",
    "    # newline='' 的作用：在写入文件时，不自动添加任何特定系统的换行符，而是由编写程序的人来决定具体使用哪种换行符；在读取文件时，统一将所有类型的换行符都识别为\\n。\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cac4c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function\n",
    "\n",
    "def check_price():\n",
    "    URL4 = 'https://www.amazon.com/Yunnan-Lucky-Dragon-Pu-erh-Chinese/dp/B016DJ2S3I/ref=sr_1_9?crid=2XQILW7VQY85L&keywords=%E6%99%AE%E6%B4%B1%E8%8C%B6&qid=1705822109&refresh=1&sprefix=%E6%99%AE%E6%B4%B1%E8%8C%B6%2Caps%2C418&sr=8-9'\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36','Accept-Encoding': 'gzip, deflate, br','Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7','DNT':'1','Connection':'close','Upgrade-Insecure-Requests':'1'}\n",
    "\n",
    "    page = requests.get(URL4, headers=headers)\n",
    "\n",
    "    Soup1 = BeautifulSoup(page.content,'html.parser')  #page.content 将会返回整个HTML源码。而page.text则只会返回文本内容\n",
    "\n",
    "    Soup2 = BeautifulSoup(Soup1.prettify(),'html.parser') #Soup1.prettify()是格式化并美化解析后的HTML或XML内容，缩进成便于阅读的情况\n",
    "\n",
    "    title = Soup2.find( id = 'productTitle' ).get_text().strip()\n",
    "\n",
    "    price = Soup2.find('span', attrs = {'class':'a-offscreen'}).get_text().strip()   #.strip是为了把前后的空格删掉\n",
    "    price = price[1:]\n",
    "\n",
    "    rating = Soup2.find('div', attrs={'id': 'averageCustomerReviews'}).find('span', attrs={'class': 'a-declarative'}).find('span', attrs={'id': 'acrPopover'}).find('span', attrs={'class': 'a-declarative'}).find('a', attrs={'class': 'a-popover-trigger a-declarative'}).find('span', attrs={'class': 'a-size-base a-color-base'}).get_text().strip()\n",
    "    \n",
    "    import datetime\n",
    "\n",
    "    today = datetime.date.today()\n",
    "    \n",
    "    import csv\n",
    "\n",
    "    header = ['Title','Price','Date','Rating',]\n",
    "    data = [title, price, today, rating]\n",
    "    \n",
    "    with open('AmazonWebScrapeDataSet.csv','a+',newline = '',encoding = 'UTF8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(data)\n",
    "    \n",
    "    if (price < 15):\n",
    "        send_mail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6129fb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    check_price()\n",
    "    time.sleep(86,400)   #单位是秒，86,400s是一天,只要不关机和保持这个网页（可最小化）以及网络连接，就能每天监控价格变化。最好在云端服务器使用\n",
    "#通常用于实时监控或定期检查某种状态（在这个例子中可能是商品价格），并在每次检查后等待一段时间再进行下一次检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20711436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  Price        Date  \\\n",
      "0  200g / 7.05oz 2006 Year Ripe Shu Top Aged Luck...  18.98  2024-01-22   \n",
      "1  200g / 7.05oz 2006 Year Ripe Shu Top Aged Luck...  18.98  2024-01-22   \n",
      "2  200g / 7.05oz 2006 Year Ripe Shu Top Aged Luck...  18.98  2024-01-22   \n",
      "\n",
      "   Rating  \n",
      "0     3.9  \n",
      "1     3.9  \n",
      "2     3.9  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\47042\\AmazonWebScrapeDataSet.csv')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c828cea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当价格低于15时，自动给我发送邮件\n",
    "def send_mail():\n",
    "    server = smtplib.SMTP_SSL('smtp.gmail.com',465)\n",
    "    server.ehlo()\n",
    "    server.ehlo()\n",
    "    server.login('402980437@qq.com',xxxxx)\n",
    "    \n",
    "    subject = 'The tea you want is below $15! Now is your chance to buy!'\n",
    "    body = 'Yanya, this is the moment we have been waiting for. Now is your chance to pick up the shirt of your dreams.'\n",
    "    \n",
    "    msg = f 'subject:{subject}\\n\\n{body}'\n",
    "    \n",
    "    server.sendmail('402980437@qq.com',msg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
